{"question": "What is new about DeepSeek?", "answer": "I don't know based on the provided context.", "contexts": ["TEXT:\n[Not found]"]}
{"question": "Are there any comparisons of DeepSeek with other models?", "answer": "Yes, there are comparisons of DeepSeek with other models. DeepSeek-V3 outperforms Llama 3.1 405B and GPT-4o on key benchmarks and achieves exceptional scores in coding and math. It also outperformed Llama 3.1 405B and Qwen 2.5 72B across the board in DeepSeek\u2019s tests. In coding, DeepSeek-V3 dominated in five of the seven benchmarks tested but lost to o1 on one of the five. On Polyglot, DeepSeek-V3 beat Claude Sonnet 3.5, though it lost to o1. In language tasks, it performed neck-and-neck with Claude 3.5 Sonnet.", "contexts": ["TEXT:\nA new model from Hangzhou upstart DeepSeek delivers outstanding performance and may change the equation for training costs.\n\nWhat\u2019s new:\u00a0DeepSeek-V3\u00a0is an open large language model that outperforms Llama 3.1 405B and GPT-4o on key benchmarks and achieves exceptional scores in coding and math. The weights are\u00a0open\u00a0except for applications that involve military uses, harming minors, generating false information, and similar restrictions. You can download them\u00a0here.\n\nMixture of experts (MoE) basics:\u00a0The MoE architecture uses different subsets of its parameters to process different inputs. Each MoE layer contains a group of neural networks, or experts, preceded by a gating module that learns to choose which one(s) to use based on the input. In this way, different experts learn to specialize in different types of examples. Because not all parameters are used to produce any given output, the network uses less energy and runs faster than models of similar size that use all parameters to process every input.\n\nHow it works:\u00a0DeepSeek-V3 is a mixture-of-experts (MoE) transformer that comprises 671 billion parameters, of which 37 billion are active at any moment. The team trained the model in 2.79 million GPU hours \u2014 less than 1/10 the\u00a0time required to train Llama 3.1 405B, which DeepSeek-V3 substantially outperforms \u2014 at an extraordinarily low cost of $5.6 million.\n\nThe developers trained it on roughly 15 trillion tokens, including a larger percentage of coding and math data relative to DeepSeek-V2. They fine-tuned it on a wide variety of tasks using output generated by\u00a0DeepSeek-R1\u00a0and DeepSeek-V2.5. They further sharpened its performance across diverse domains using the reinforcement learning algorithm known as\u00a0group relative policy optimization. \u00a0Earlier\u00a0work\u00a0showed that training to predict the next two tokens would improve performance over learning to predict just one. The authors implemented this procedure. The model learned to predict the first token as usual and used an additional set of layers to learn to predict the second token. The additional layers aren\u2019t used at inference.Following\u00a0DeepSeek-V2, DeepSeek-V3 uses multi-head latent attention, which saves memory during execution relative to other variants of attention.Also like DeepSeek-V2, the new model combines dedicated (routed) and shared experts. The model chooses eight of 256 experts for a particular input, but it also uses a shared expert that processes all inputs.\n\nResults:\u00a0In DeepSeek\u2019s tests, DeepSeek-V3 outperformed Llama 3.1 405B and Qwen 2.5 72B across the board, and its performance compared favorably with that of GPT-4o.\n\nDeepSeek-V3 showed exceptional performance in coding and math tasks. In coding, DeepSeek-V3 dominated in five of the seven benchmarks tested. However, DeepSeek-V3 lost to o1 on one of the five, according to a public leaderboard. Specifically, on\u00a0Polyglot, which tests a model\u2019s ability to generate code in response to difficult requests in multiple programming languages, DeepSeek-V3 (48.5 percent accuracy) beat Claude Sonnet 3.5 (45.3 percent accuracy), though it lost to o1 (61.7 percent accuracy).\u00a0In language tasks, it performed neck-and-neck with Claude 3.5 Sonnet, achieving higher scores in some tasks and lower in others.\n\nBehind the news:\u00a0\u00a0OpenAI\u2019s o1 models excel thanks to agentic workflows in which they reflect on their own outputs, use tools, and so on. DeepSeek swims against the tide and achieves superior results without relying on agentic workflows.\n\nWhy it matters:\u00a0Open models continue to challenge closed models, giving developers high-quality options that they can modify and deploy at will. But the larger story is DeepSeek-V3\u2019s shockingly low training cost.\u00a0The team doesn\u2019t explain precisely how the model achieves outstanding performance with such a low processing budget. (The paper credits \u201cmeticulous engineering optimizations.\u201d) But it\u2019s likely that DeepSeek\u2019s steady refinement of MoE is a key factor. DeepSeek-V2, also an MoE model, saved more than 40 percent in training versus the earlier DeepSeek 67B, which didn\u2019t employ MoE. In 2022,\u00a0Microsoft\u00a0found that MoE cost five times less in training for equal performance compared to a dense model, and\u00a0Google\u00a0and\u00a0Meta\u00a0reported that MoE achieved better performance than dense models trained on the same numbers of tokens.\n\nWe\u2019re thinking:\u00a0If they can be replicated, DeepSeek\u2019s results have significant implications for the economics of training foundation models. If indeed it now costs around $5 million to build a GPT-4o-level model, more teams will be able to train such models, and the cost of competing with the AI giants could fall dramatically."]}
{"question": "Are there any updates on Deep Seek 1 comparison to other models?", "answer": "The text does not provide any information on how Deep Seek 1 compares to other models.", "contexts": ["TEXT:\nA new model from Hangzhou upstart DeepSeek delivers outstanding performance and may change the equation for training costs.\n\nWhat\u2019s new:\u00a0DeepSeek-V3\u00a0is an open large language model that outperforms Llama 3.1 405B and GPT-4o on key benchmarks and achieves exceptional scores in coding and math. The weights are\u00a0open\u00a0except for applications that involve military uses, harming minors, generating false information, and similar restrictions. You can download them\u00a0here.\n\nMixture of experts (MoE) basics:\u00a0The MoE architecture uses different subsets of its parameters to process different inputs. Each MoE layer contains a group of neural networks, or experts, preceded by a gating module that learns to choose which one(s) to use based on the input. In this way, different experts learn to specialize in different types of examples. Because not all parameters are used to produce any given output, the network uses less energy and runs faster than models of similar size that use all parameters to process every input.\n\nHow it works:\u00a0DeepSeek-V3 is a mixture-of-experts (MoE) transformer that comprises 671 billion parameters, of which 37 billion are active at any moment. The team trained the model in 2.79 million GPU hours \u2014 less than 1/10 the\u00a0time required to train Llama 3.1 405B, which DeepSeek-V3 substantially outperforms \u2014 at an extraordinarily low cost of $5.6 million.\n\nThe developers trained it on roughly 15 trillion tokens, including a larger percentage of coding and math data relative to DeepSeek-V2. They fine-tuned it on a wide variety of tasks using output generated by\u00a0DeepSeek-R1\u00a0and DeepSeek-V2.5. They further sharpened its performance across diverse domains using the reinforcement learning algorithm known as\u00a0group relative policy optimization. \u00a0Earlier\u00a0work\u00a0showed that training to predict the next two tokens would improve performance over learning to predict just one. The authors implemented this procedure. The model learned to predict the first token as usual and used an additional set of layers to learn to predict the second token. The additional layers aren\u2019t used at inference.Following\u00a0DeepSeek-V2, DeepSeek-V3 uses multi-head latent attention, which saves memory during execution relative to other variants of attention.Also like DeepSeek-V2, the new model combines dedicated (routed) and shared experts. The model chooses eight of 256 experts for a particular input, but it also uses a shared expert that processes all inputs.\n\nResults:\u00a0In DeepSeek\u2019s tests, DeepSeek-V3 outperformed Llama 3.1 405B and Qwen 2.5 72B across the board, and its performance compared favorably with that of GPT-4o.\n\nDeepSeek-V3 showed exceptional performance in coding and math tasks. In coding, DeepSeek-V3 dominated in five of the seven benchmarks tested. However, DeepSeek-V3 lost to o1 on one of the five, according to a public leaderboard. Specifically, on\u00a0Polyglot, which tests a model\u2019s ability to generate code in response to difficult requests in multiple programming languages, DeepSeek-V3 (48.5 percent accuracy) beat Claude Sonnet 3.5 (45.3 percent accuracy), though it lost to o1 (61.7 percent accuracy).\u00a0In language tasks, it performed neck-and-neck with Claude 3.5 Sonnet, achieving higher scores in some tasks and lower in others.\n\nBehind the news:\u00a0\u00a0OpenAI\u2019s o1 models excel thanks to agentic workflows in which they reflect on their own outputs, use tools, and so on. DeepSeek swims against the tide and achieves superior results without relying on agentic workflows.\n\nWhy it matters:\u00a0Open models continue to challenge closed models, giving developers high-quality options that they can modify and deploy at will. But the larger story is DeepSeek-V3\u2019s shockingly low training cost.\u00a0The team doesn\u2019t explain precisely how the model achieves outstanding performance with such a low processing budget. (The paper credits \u201cmeticulous engineering optimizations.\u201d) But it\u2019s likely that DeepSeek\u2019s steady refinement of MoE is a key factor. DeepSeek-V2, also an MoE model, saved more than 40 percent in training versus the earlier DeepSeek 67B, which didn\u2019t employ MoE. In 2022,\u00a0Microsoft\u00a0found that MoE cost five times less in training for equal performance compared to a dense model, and\u00a0Google\u00a0and\u00a0Meta\u00a0reported that MoE achieved better performance than dense models trained on the same numbers of tokens.\n\nWe\u2019re thinking:\u00a0If they can be replicated, DeepSeek\u2019s results have significant implications for the economics of training foundation models. If indeed it now costs around $5 million to build a GPT-4o-level model, more teams will be able to train such models, and the cost of competing with the AI giants could fall dramatically."]}
{"question": "What is Deep Seek 1 perfomance compared to other models?", "answer": "The text does not provide information on the performance of Deep Seek 1 compared to other models.", "contexts": ["TEXT:\nA new model from Hangzhou upstart DeepSeek delivers outstanding performance and may change the equation for training costs.\n\nWhat\u2019s new:\u00a0DeepSeek-V3\u00a0is an open large language model that outperforms Llama 3.1 405B and GPT-4o on key benchmarks and achieves exceptional scores in coding and math. The weights are\u00a0open\u00a0except for applications that involve military uses, harming minors, generating false information, and similar restrictions. You can download them\u00a0here.\n\nMixture of experts (MoE) basics:\u00a0The MoE architecture uses different subsets of its parameters to process different inputs. Each MoE layer contains a group of neural networks, or experts, preceded by a gating module that learns to choose which one(s) to use based on the input. In this way, different experts learn to specialize in different types of examples. Because not all parameters are used to produce any given output, the network uses less energy and runs faster than models of similar size that use all parameters to process every input.\n\nHow it works:\u00a0DeepSeek-V3 is a mixture-of-experts (MoE) transformer that comprises 671 billion parameters, of which 37 billion are active at any moment. The team trained the model in 2.79 million GPU hours \u2014 less than 1/10 the\u00a0time required to train Llama 3.1 405B, which DeepSeek-V3 substantially outperforms \u2014 at an extraordinarily low cost of $5.6 million.\n\nThe developers trained it on roughly 15 trillion tokens, including a larger percentage of coding and math data relative to DeepSeek-V2. They fine-tuned it on a wide variety of tasks using output generated by\u00a0DeepSeek-R1\u00a0and DeepSeek-V2.5. They further sharpened its performance across diverse domains using the reinforcement learning algorithm known as\u00a0group relative policy optimization. \u00a0Earlier\u00a0work\u00a0showed that training to predict the next two tokens would improve performance over learning to predict just one. The authors implemented this procedure. The model learned to predict the first token as usual and used an additional set of layers to learn to predict the second token. The additional layers aren\u2019t used at inference.Following\u00a0DeepSeek-V2, DeepSeek-V3 uses multi-head latent attention, which saves memory during execution relative to other variants of attention.Also like DeepSeek-V2, the new model combines dedicated (routed) and shared experts. The model chooses eight of 256 experts for a particular input, but it also uses a shared expert that processes all inputs.\n\nResults:\u00a0In DeepSeek\u2019s tests, DeepSeek-V3 outperformed Llama 3.1 405B and Qwen 2.5 72B across the board, and its performance compared favorably with that of GPT-4o.\n\nDeepSeek-V3 showed exceptional performance in coding and math tasks. In coding, DeepSeek-V3 dominated in five of the seven benchmarks tested. However, DeepSeek-V3 lost to o1 on one of the five, according to a public leaderboard. Specifically, on\u00a0Polyglot, which tests a model\u2019s ability to generate code in response to difficult requests in multiple programming languages, DeepSeek-V3 (48.5 percent accuracy) beat Claude Sonnet 3.5 (45.3 percent accuracy), though it lost to o1 (61.7 percent accuracy).\u00a0In language tasks, it performed neck-and-neck with Claude 3.5 Sonnet, achieving higher scores in some tasks and lower in others.\n\nBehind the news:\u00a0\u00a0OpenAI\u2019s o1 models excel thanks to agentic workflows in which they reflect on their own outputs, use tools, and so on. DeepSeek swims against the tide and achieves superior results without relying on agentic workflows.\n\nWhy it matters:\u00a0Open models continue to challenge closed models, giving developers high-quality options that they can modify and deploy at will. But the larger story is DeepSeek-V3\u2019s shockingly low training cost.\u00a0The team doesn\u2019t explain precisely how the model achieves outstanding performance with such a low processing budget. (The paper credits \u201cmeticulous engineering optimizations.\u201d) But it\u2019s likely that DeepSeek\u2019s steady refinement of MoE is a key factor. DeepSeek-V2, also an MoE model, saved more than 40 percent in training versus the earlier DeepSeek 67B, which didn\u2019t employ MoE. In 2022,\u00a0Microsoft\u00a0found that MoE cost five times less in training for equal performance compared to a dense model, and\u00a0Google\u00a0and\u00a0Meta\u00a0reported that MoE achieved better performance than dense models trained on the same numbers of tokens.\n\nWe\u2019re thinking:\u00a0If they can be replicated, DeepSeek\u2019s results have significant implications for the economics of training foundation models. If indeed it now costs around $5 million to build a GPT-4o-level model, more teams will be able to train such models, and the cost of competing with the AI giants could fall dramatically."]}
{"question": "What is DeepSeek 1 perfomance compared to other models?", "answer": "The context does not provide information on the performance of DeepSeek 1 compared to other models.", "contexts": ["TEXT:\nA new model from Hangzhou upstart DeepSeek delivers outstanding performance and may change the equation for training costs.\n\nWhat\u2019s new:\u00a0DeepSeek-V3\u00a0is an open large language model that outperforms Llama 3.1 405B and GPT-4o on key benchmarks and achieves exceptional scores in coding and math. The weights are\u00a0open\u00a0except for applications that involve military uses, harming minors, generating false information, and similar restrictions. You can download them\u00a0here.\n\nMixture of experts (MoE) basics:\u00a0The MoE architecture uses different subsets of its parameters to process different inputs. Each MoE layer contains a group of neural networks, or experts, preceded by a gating module that learns to choose which one(s) to use based on the input. In this way, different experts learn to specialize in different types of examples. Because not all parameters are used to produce any given output, the network uses less energy and runs faster than models of similar size that use all parameters to process every input.\n\nHow it works:\u00a0DeepSeek-V3 is a mixture-of-experts (MoE) transformer that comprises 671 billion parameters, of which 37 billion are active at any moment. The team trained the model in 2.79 million GPU hours \u2014 less than 1/10 the\u00a0time required to train Llama 3.1 405B, which DeepSeek-V3 substantially outperforms \u2014 at an extraordinarily low cost of $5.6 million.\n\nThe developers trained it on roughly 15 trillion tokens, including a larger percentage of coding and math data relative to DeepSeek-V2. They fine-tuned it on a wide variety of tasks using output generated by\u00a0DeepSeek-R1\u00a0and DeepSeek-V2.5. They further sharpened its performance across diverse domains using the reinforcement learning algorithm known as\u00a0group relative policy optimization. \u00a0Earlier\u00a0work\u00a0showed that training to predict the next two tokens would improve performance over learning to predict just one. The authors implemented this procedure. The model learned to predict the first token as usual and used an additional set of layers to learn to predict the second token. The additional layers aren\u2019t used at inference.Following\u00a0DeepSeek-V2, DeepSeek-V3 uses multi-head latent attention, which saves memory during execution relative to other variants of attention.Also like DeepSeek-V2, the new model combines dedicated (routed) and shared experts. The model chooses eight of 256 experts for a particular input, but it also uses a shared expert that processes all inputs.\n\nResults:\u00a0In DeepSeek\u2019s tests, DeepSeek-V3 outperformed Llama 3.1 405B and Qwen 2.5 72B across the board, and its performance compared favorably with that of GPT-4o.\n\nDeepSeek-V3 showed exceptional performance in coding and math tasks. In coding, DeepSeek-V3 dominated in five of the seven benchmarks tested. However, DeepSeek-V3 lost to o1 on one of the five, according to a public leaderboard. Specifically, on\u00a0Polyglot, which tests a model\u2019s ability to generate code in response to difficult requests in multiple programming languages, DeepSeek-V3 (48.5 percent accuracy) beat Claude Sonnet 3.5 (45.3 percent accuracy), though it lost to o1 (61.7 percent accuracy).\u00a0In language tasks, it performed neck-and-neck with Claude 3.5 Sonnet, achieving higher scores in some tasks and lower in others.\n\nBehind the news:\u00a0\u00a0OpenAI\u2019s o1 models excel thanks to agentic workflows in which they reflect on their own outputs, use tools, and so on. DeepSeek swims against the tide and achieves superior results without relying on agentic workflows.\n\nWhy it matters:\u00a0Open models continue to challenge closed models, giving developers high-quality options that they can modify and deploy at will. But the larger story is DeepSeek-V3\u2019s shockingly low training cost.\u00a0The team doesn\u2019t explain precisely how the model achieves outstanding performance with such a low processing budget. (The paper credits \u201cmeticulous engineering optimizations.\u201d) But it\u2019s likely that DeepSeek\u2019s steady refinement of MoE is a key factor. DeepSeek-V2, also an MoE model, saved more than 40 percent in training versus the earlier DeepSeek 67B, which didn\u2019t employ MoE. In 2022,\u00a0Microsoft\u00a0found that MoE cost five times less in training for equal performance compared to a dense model, and\u00a0Google\u00a0and\u00a0Meta\u00a0reported that MoE achieved better performance than dense models trained on the same numbers of tokens.\n\nWe\u2019re thinking:\u00a0If they can be replicated, DeepSeek\u2019s results have significant implications for the economics of training foundation models. If indeed it now costs around $5 million to build a GPT-4o-level model, more teams will be able to train such models, and the cost of competing with the AI giants could fall dramatically."]}
{"question": "What is new about deepseek v3 perfomance comparing to other models?", "answer": "The new DeepSeek-R1-0528 surpasses its predecessor and approaches the performance of OpenAI o3 and Google Gemini-2.5 Pro. It has been fine-tuned from DeepSeek-V3 and exposed to further algorithmic optimization mechanisms during post-training. This model consumes more tokens at inference and has shown improvements in reasoning, managing complex tasks, and writing and editing lengthy prose. It also has 50 percent fewer hallucinations when rewriting and summarizing. In terms of performance, DeepSeek-R1-0528 performs ahead of Gemini-2.5 Pro-0506 but behind o3 on some tasks, and falls behind both on others. A smaller version, DeepSeek-R1-0528-Qwen3-8B, runs on a single GPU with as little as 40GB VRAM and raises the bar for LLMs in its 8-billion-parameter size class. It excelled on AIME 2025, achieving 76.3 percent, ahead of the much larger Qwen3-32B and just behind o3-mini set to medium effort.", "contexts": ["TEXT:\nDeepSeek updated its groundbreaking DeepSeek-R1 large language model to strike another blow for open-weights performance.\n\nWhat\u2019s new:\u00a0The new\u00a0DeepSeek-R1-0528\u00a0surpasses its predecessor and approaches the performance of OpenAI o3 and Google Gemini-2.5 Pro. A smaller\u00a0version,\u00a0DeepSeek-R1-0528-Qwen3-8B, runs on a single GPU with as little as 40GB VRAM, according to\u00a0TechCrunch.\n\nInput/output:\u00a0Text in (up to 64,000 tokens), text out (up to 64,000 tokens)Architecture:\u00a0DeepSeek-R1-0528\u00a0mixture-of-experts transformer, 685 billion parameters (upgraded from 671 billion), 37 billion active at any given time;\u00a0DeepSeek-R1-0528-Qwen3-8B\u00a0transformerFeatures:\u00a0JSON output, tool useAvailability/price:\u00a0Both models free via\u00a0Hugging Face\u00a0for noncommercial and commercial uses under\u00a0MIT License, DeepSeek-R1-0528 available via DeepSeek\u2019s app by entering the conversation interface and turning on Deep Thinking,\u00a0DeepSeek API\u00a0$0.14/$2.19 per 1 million tokens of input/output ($0.035/$0.55 per 1 million tokens of input/output from 4:30 P.M. to 12:30 A.M. Pacific Time)Undisclosed:\u00a0Fine-tuning data and methods\n\nHow it works:\u00a0DeepSeek released little information so far about how it built the new models.\n\nLike the original\u00a0DeepSeek-R1, DeepSeek-R1-0528 is a fine-tuned version of\u00a0DeepSeek-V3\u00a0from late 2024. It was exposed to further \u201calgorithmic optimization mechanisms during post-training\u201d and consumes more tokens at inference.DeepSeek-R1-0528-Qwen3-8B is based on Qwen3-8B with reasoning knowledge distilled from DeepSeek-R1-0528.\n\nPerformance:\u00a0DeepSeek-R1-0528 nips at the heels of top closed LLMs on a variety of benchmarks, while DeepSeek-R1-0528-Qwen3-8B raises the bar for LLMs in its 8-billion-parameter size class. DeepSeek claims general improvements in reasoning, managing complex tasks, and writing and editing lengthy prose, along with 50 percent fewer hallucinations when rewriting and summarizing.\n\nDeepSeek-R1-0528 improves on the previous version dramatically in some cases. In DeepSeek\u2019s tests, it achieved 17.7 percent of the reasoning problems in\u00a0HLE\u00a0compared to the previous version's 8.5 percent. On Aider, it achieved 71.6 percent accuracy compared to the previous version's 53.3 percent accuracy, and it made a similar improvement on AIME 2025 (math) \u2014 although it consumed nearly twice as many tokens.On AIME 2024 and AIME 2025 (high-school math competition problems) as well as\u00a0LiveCodeBench\u00a0(coding challenges), DeepSeek-R1-0528 performed ahead of Gemini-2.5 Pro-0506 but behind o3. On GPQA Diamond (graduate-level knowledge in a variety of domains), Aider (programming tasks), and HLE (reasoning), it\u00a0fell behind both Gemini-2.5 Pro-0506 and o3.DeepSeek-R1-0528-Qwen3-8B excelled on AIME 2025, where it achieved 76.3 percent, ahead of the much larger Qwen3-32B (72.9 percent) and just behind o3-mini set to medium effort (76.7 percent). It did less well on GPQA, underperforming\u00a0the other models reported by DeepSeek, and LiveCodeBench, where it fell behind Gemini 2.5-Flash-Thinking-0520.\n\nBehind the news:\u00a0The initial version of DeepSeek-R1 challenged the belief that building top-performing AI models requires tens to hundreds of millions of dollars, top-of-the-line GPUs, and enormous numbers of GPU hours. For the second time in less than a year, DeepSeek has built a competitive LLM with a relatively\u00a0low\u00a0budget.\n\nWhy it matters:\u00a0DeepSeek\u2019s models, along with Alibaba\u2019s Qwen series, continue\u00a0to narrow the gap between open-weights models and their closed peers. Its accomplishments could lead to wider adoption of less-expensive, more-efficient approaches. DeepSeek is passing along the cost savings to developers, offering high-performance inference at a fraction of the cost of closed models.\n\nWe\u2019re thinking:\u00a0DeepSeek-R1-0528-Qwen3-8B mixes contributions from open-weight models \u2014 possible only because Qwen3\u2019s license, like DeepSeek\u2019s is permissive. Open models enable experimentation and innovation in ways that closed models do not."]}
{"question": "What is new about deepseek v3, is it better then other models?", "answer": "DeepSeek-V3 is a new model that delivers outstanding performance and may change the equation for training costs. It is an open large language model that outperforms Llama 3.1 405B and GPT-4o on key benchmarks and achieves exceptional scores in coding and math. The model is a mixture-of-experts (MoE) transformer that comprises 671 billion parameters, of which 37 billion are active at any moment. It was trained in less time and at a lower cost than the Llama 3.1 405B model. The model also showed exceptional performance in coding and math tasks, dominating in five of the seven benchmarks tested. However, it did lose to o1 on one of the five, according to a public leaderboard. In language tasks, it performed neck-and-neck with Claude 3.5 Sonnet. The larger story is DeepSeek-V3\u2019s shockingly low training cost, which could have significant implications for the economics of training foundation models.", "contexts": ["TEXT:\nA new model from Hangzhou upstart DeepSeek delivers outstanding performance and may change the equation for training costs.\n\nWhat\u2019s new:\u00a0DeepSeek-V3\u00a0is an open large language model that outperforms Llama 3.1 405B and GPT-4o on key benchmarks and achieves exceptional scores in coding and math. The weights are\u00a0open\u00a0except for applications that involve military uses, harming minors, generating false information, and similar restrictions. You can download them\u00a0here.\n\nMixture of experts (MoE) basics:\u00a0The MoE architecture uses different subsets of its parameters to process different inputs. Each MoE layer contains a group of neural networks, or experts, preceded by a gating module that learns to choose which one(s) to use based on the input. In this way, different experts learn to specialize in different types of examples. Because not all parameters are used to produce any given output, the network uses less energy and runs faster than models of similar size that use all parameters to process every input.\n\nHow it works:\u00a0DeepSeek-V3 is a mixture-of-experts (MoE) transformer that comprises 671 billion parameters, of which 37 billion are active at any moment. The team trained the model in 2.79 million GPU hours \u2014 less than 1/10 the\u00a0time required to train Llama 3.1 405B, which DeepSeek-V3 substantially outperforms \u2014 at an extraordinarily low cost of $5.6 million.\n\nThe developers trained it on roughly 15 trillion tokens, including a larger percentage of coding and math data relative to DeepSeek-V2. They fine-tuned it on a wide variety of tasks using output generated by\u00a0DeepSeek-R1\u00a0and DeepSeek-V2.5. They further sharpened its performance across diverse domains using the reinforcement learning algorithm known as\u00a0group relative policy optimization. \u00a0Earlier\u00a0work\u00a0showed that training to predict the next two tokens would improve performance over learning to predict just one. The authors implemented this procedure. The model learned to predict the first token as usual and used an additional set of layers to learn to predict the second token. The additional layers aren\u2019t used at inference.Following\u00a0DeepSeek-V2, DeepSeek-V3 uses multi-head latent attention, which saves memory during execution relative to other variants of attention.Also like DeepSeek-V2, the new model combines dedicated (routed) and shared experts. The model chooses eight of 256 experts for a particular input, but it also uses a shared expert that processes all inputs.\n\nResults:\u00a0In DeepSeek\u2019s tests, DeepSeek-V3 outperformed Llama 3.1 405B and Qwen 2.5 72B across the board, and its performance compared favorably with that of GPT-4o.\n\nDeepSeek-V3 showed exceptional performance in coding and math tasks. In coding, DeepSeek-V3 dominated in five of the seven benchmarks tested. However, DeepSeek-V3 lost to o1 on one of the five, according to a public leaderboard. Specifically, on\u00a0Polyglot, which tests a model\u2019s ability to generate code in response to difficult requests in multiple programming languages, DeepSeek-V3 (48.5 percent accuracy) beat Claude Sonnet 3.5 (45.3 percent accuracy), though it lost to o1 (61.7 percent accuracy).\u00a0In language tasks, it performed neck-and-neck with Claude 3.5 Sonnet, achieving higher scores in some tasks and lower in others.\n\nBehind the news:\u00a0\u00a0OpenAI\u2019s o1 models excel thanks to agentic workflows in which they reflect on their own outputs, use tools, and so on. DeepSeek swims against the tide and achieves superior results without relying on agentic workflows.\n\nWhy it matters:\u00a0Open models continue to challenge closed models, giving developers high-quality options that they can modify and deploy at will. But the larger story is DeepSeek-V3\u2019s shockingly low training cost.\u00a0The team doesn\u2019t explain precisely how the model achieves outstanding performance with such a low processing budget. (The paper credits \u201cmeticulous engineering optimizations.\u201d) But it\u2019s likely that DeepSeek\u2019s steady refinement of MoE is a key factor. DeepSeek-V2, also an MoE model, saved more than 40 percent in training versus the earlier DeepSeek 67B, which didn\u2019t employ MoE. In 2022,\u00a0Microsoft\u00a0found that MoE cost five times less in training for equal performance compared to a dense model, and\u00a0Google\u00a0and\u00a0Meta\u00a0reported that MoE achieved better performance than dense models trained on the same numbers of tokens.\n\nWe\u2019re thinking:\u00a0If they can be replicated, DeepSeek\u2019s results have significant implications for the economics of training foundation models. If indeed it now costs around $5 million to build a GPT-4o-level model, more teams will be able to train such models, and the cost of competing with the AI giants could fall dramatically."]}
{"question": "Do we have any news on LA protests?", "answer": "I don't know based on the provided context.", "contexts": ["TEXT:\nA study co-authored by tech-manual publisher Tim O\u2019Reilly shows that OpenAI trained GPT-4o on parts of his company\u2019s books that were not made freely available.\n\nWhat happened:\u00a0O\u2019Reilly, computer scientist Sruly Rosenblat, and economist Ilan Strauss\u00a0found\u00a0that GPT-4o was able to identify verbatim excerpts from dozens of O\u2019Reilly Media books that the company kept behind a paywall, indicating that the books likely were included in the model\u2019s training data.\n\nHow it works:\u00a0The researchers adapted the\u00a0DE-COP\u00a0method to compare how well GPT-4o, GPT-4o-mini, and GPT-3.5 Turbo recognized paywalled excerpts versus freely available excerpts from the same books.\n\nThe team selected 34 O\u2019Reilly Media books and divided them into roughly 14,000 paragraphs.They labeled the paragraphs private (paywalled) or public (when O\u2019Reilly Media publishes a book, it distributes freely on the web chapters 1 and 4 as well as the first 1,500 characters of other chapters). They also labeled the paragraphs according to whether they were published before or after the models\u2019 knowledge cutoff dates.The team built multiple-choice quizzes, each composed of a verbatim paragraph and three paraphrased versions generated by Claude 3.5 Sonnet. The researchers ordered the paragraphs and paraphrases in all permutations to eliminate potential position bias.\n\nResults:\u00a0The authors asked each model to identify the verbatim paragraph and calculated each model\u2019s percentage of correct responses. Then they averaged each model\u2019s accuracy per book and converted the averages into AUROC scores that measure how well a model distinguished books available prior to its knowledge cutoff (potentially included in the training set) from those that weren\u2019t available at the time. 50 percent AUROC indicates random chance, while higher scores indicate higher accuracy.\n\nGPT-4o tended to recognize O\u2019Reilly Media content whether or not it was public, but it recognized private paragraphs (82 percent AUROC) markedly more often than public paragraphs (64 percent AUROC).GPT-4o-mini\u2019s performance was nearly random for both private (56% AUROC) and public material (55% AUROC). The researchers hypothesize that either (i) the model\u2019s smaller size may limit its ability to memorize or (2) OpenAI may reserve premium data to train larger models.The earlier GPT-3.5 Turbo recognized public paragraphs (64% AUROC) more often than private paragraphs (54% AUROC), which suggests that it was trained predominantly on freely available data.\n\nYes, but:\u00a0Newer large language models are better at distinguishing human-written from generated text, even if it wasn\u2019t in their training sets. For instance, given paragraphs that were published after their knowledge cutoffs, GPT-4o returned scores as high as 78 percent AUROC. The authors note that this may challenge their conclusions, since they interpret high scores to indicate that a model saw the text during training. Nonetheless, they argue that their approach will remain valid while\u00a0scores for both text that was included and text that was excluded from training sets\u00a0remain under 96 percent AUROC. \u201cFor now,\u201d they write, \u201cthe gap remains sufficiently large to reliably separate the two categories.\u201d\n\nBehind the news:\u00a0Historically AI developers have trained machine learning models on any data they could acquire. But in the era of generative AI, models trained on copyrighted works can mimic the works and styles of the works\u2019 owners, creating a threat to their livelihoods. Some AI developers have responded by regarding data that\u2019s freely available on the web as fair game, and material that\u2019s otherwise protected as off-limits for training. However, datasets that include ostensibly private data are widely circulated, including\u00a0LibGen, which includes all 34 of the O\u2019Reilly Media titles tested in this study. Moreover, unauthorized copies of many copyrighted works are posted\u00a0without paywalls or even logins, making it possible even\u00a0for web scrapers that crawl only the open web to download them. Google and OpenAI, which is currently embroiled in lawsuits by authors and publishers who claim it violated copyrights by training models on copyrighted works, recently\u00a0lobbied\u00a0the United States government to relax copyright laws for AI developers.\n\nWhy it matters:\u00a0The AI industry requires huge quantities of high-quality data to keep advancing the state of the art. At the same time, copyright owners are worried that models trained on their data might hamper their opportunities to earn a living. AI developers must find fair ways to respond. As O\u2019Reilly points out, exploiting copyrighted works instead of rewarding their authors could lead to an \u201cextractive dead end\u201d that ultimately diminishes the supply of the high-quality training data.\n\nWe\u2019re thinking:\u00a0We have learned a great deal from O\u2019Reilly Media\u2019s books, and we\u2019re grateful to the many authors, editors, graphic artists, and others who produce them. Meanwhile, it\u2019s time for the U.S. Congress \u2014\u00a0 and legislators internationally \u2014 to\u00a0update\u00a0copyright laws for the era of generative AI, so everyone knows the rules and we can find ways to follow them."]}
{"question": "Do we have any news on LA protests?", "answer": "I don't know based on the provided context.", "contexts": ["TEXT:\nA study co-authored by tech-manual publisher Tim O\u2019Reilly shows that OpenAI trained GPT-4o on parts of his company\u2019s books that were not made freely available.\n\nWhat happened:\u00a0O\u2019Reilly, computer scientist Sruly Rosenblat, and economist Ilan Strauss\u00a0found\u00a0that GPT-4o was able to identify verbatim excerpts from dozens of O\u2019Reilly Media books that the company kept behind a paywall, indicating that the books likely were included in the model\u2019s training data.\n\nHow it works:\u00a0The researchers adapted the\u00a0DE-COP\u00a0method to compare how well GPT-4o, GPT-4o-mini, and GPT-3.5 Turbo recognized paywalled excerpts versus freely available excerpts from the same books.\n\nThe team selected 34 O\u2019Reilly Media books and divided them into roughly 14,000 paragraphs.They labeled the paragraphs private (paywalled) or public (when O\u2019Reilly Media publishes a book, it distributes freely on the web chapters 1 and 4 as well as the first 1,500 characters of other chapters). They also labeled the paragraphs according to whether they were published before or after the models\u2019 knowledge cutoff dates.The team built multiple-choice quizzes, each composed of a verbatim paragraph and three paraphrased versions generated by Claude 3.5 Sonnet. The researchers ordered the paragraphs and paraphrases in all permutations to eliminate potential position bias.\n\nResults:\u00a0The authors asked each model to identify the verbatim paragraph and calculated each model\u2019s percentage of correct responses. Then they averaged each model\u2019s accuracy per book and converted the averages into AUROC scores that measure how well a model distinguished books available prior to its knowledge cutoff (potentially included in the training set) from those that weren\u2019t available at the time. 50 percent AUROC indicates random chance, while higher scores indicate higher accuracy.\n\nGPT-4o tended to recognize O\u2019Reilly Media content whether or not it was public, but it recognized private paragraphs (82 percent AUROC) markedly more often than public paragraphs (64 percent AUROC).GPT-4o-mini\u2019s performance was nearly random for both private (56% AUROC) and public material (55% AUROC). The researchers hypothesize that either (i) the model\u2019s smaller size may limit its ability to memorize or (2) OpenAI may reserve premium data to train larger models.The earlier GPT-3.5 Turbo recognized public paragraphs (64% AUROC) more often than private paragraphs (54% AUROC), which suggests that it was trained predominantly on freely available data.\n\nYes, but:\u00a0Newer large language models are better at distinguishing human-written from generated text, even if it wasn\u2019t in their training sets. For instance, given paragraphs that were published after their knowledge cutoffs, GPT-4o returned scores as high as 78 percent AUROC. The authors note that this may challenge their conclusions, since they interpret high scores to indicate that a model saw the text during training. Nonetheless, they argue that their approach will remain valid while\u00a0scores for both text that was included and text that was excluded from training sets\u00a0remain under 96 percent AUROC. \u201cFor now,\u201d they write, \u201cthe gap remains sufficiently large to reliably separate the two categories.\u201d\n\nBehind the news:\u00a0Historically AI developers have trained machine learning models on any data they could acquire. But in the era of generative AI, models trained on copyrighted works can mimic the works and styles of the works\u2019 owners, creating a threat to their livelihoods. Some AI developers have responded by regarding data that\u2019s freely available on the web as fair game, and material that\u2019s otherwise protected as off-limits for training. However, datasets that include ostensibly private data are widely circulated, including\u00a0LibGen, which includes all 34 of the O\u2019Reilly Media titles tested in this study. Moreover, unauthorized copies of many copyrighted works are posted\u00a0without paywalls or even logins, making it possible even\u00a0for web scrapers that crawl only the open web to download them. Google and OpenAI, which is currently embroiled in lawsuits by authors and publishers who claim it violated copyrights by training models on copyrighted works, recently\u00a0lobbied\u00a0the United States government to relax copyright laws for AI developers.\n\nWhy it matters:\u00a0The AI industry requires huge quantities of high-quality data to keep advancing the state of the art. At the same time, copyright owners are worried that models trained on their data might hamper their opportunities to earn a living. AI developers must find fair ways to respond. As O\u2019Reilly points out, exploiting copyrighted works instead of rewarding their authors could lead to an \u201cextractive dead end\u201d that ultimately diminishes the supply of the high-quality training data.\n\nWe\u2019re thinking:\u00a0We have learned a great deal from O\u2019Reilly Media\u2019s books, and we\u2019re grateful to the many authors, editors, graphic artists, and others who produce them. Meanwhile, it\u2019s time for the U.S. Congress \u2014\u00a0 and legislators internationally \u2014 to\u00a0update\u00a0copyright laws for the era of generative AI, so everyone knows the rules and we can find ways to follow them."]}
{"question": "Do we have any news on LA protests?", "answer": "I don't know based on the provided context.", "contexts": [""]}
{"question": "Do we have any news on LA protests?", "answer": "I don't know based on the provided context.", "contexts": ["TEXT:\nA study co-authored by tech-manual publisher Tim O\u2019Reilly shows that OpenAI trained GPT-4o on parts of his company\u2019s books that were not made freely available.\n\nWhat happened:\u00a0O\u2019Reilly, computer scientist Sruly Rosenblat, and economist Ilan Strauss\u00a0found\u00a0that GPT-4o was able to identify verbatim excerpts from dozens of O\u2019Reilly Media books that the company kept behind a paywall, indicating that the books likely were included in the model\u2019s training data.\n\nHow it works:\u00a0The researchers adapted the\u00a0DE-COP\u00a0method to compare how well GPT-4o, GPT-4o-mini, and GPT-3.5 Turbo recognized paywalled excerpts versus freely available excerpts from the same books.\n\nThe team selected 34 O\u2019Reilly Media books and divided them into roughly 14,000 paragraphs.They labeled the paragraphs private (paywalled) or public (when O\u2019Reilly Media publishes a book, it distributes freely on the web chapters 1 and 4 as well as the first 1,500 characters of other chapters). They also labeled the paragraphs according to whether they were published before or after the models\u2019 knowledge cutoff dates.The team built multiple-choice quizzes, each composed of a verbatim paragraph and three paraphrased versions generated by Claude 3.5 Sonnet. The researchers ordered the paragraphs and paraphrases in all permutations to eliminate potential position bias.\n\nResults:\u00a0The authors asked each model to identify the verbatim paragraph and calculated each model\u2019s percentage of correct responses. Then they averaged each model\u2019s accuracy per book and converted the averages into AUROC scores that measure how well a model distinguished books available prior to its knowledge cutoff (potentially included in the training set) from those that weren\u2019t available at the time. 50 percent AUROC indicates random chance, while higher scores indicate higher accuracy.\n\nGPT-4o tended to recognize O\u2019Reilly Media content whether or not it was public, but it recognized private paragraphs (82 percent AUROC) markedly more often than public paragraphs (64 percent AUROC).GPT-4o-mini\u2019s performance was nearly random for both private (56% AUROC) and public material (55% AUROC). The researchers hypothesize that either (i) the model\u2019s smaller size may limit its ability to memorize or (2) OpenAI may reserve premium data to train larger models.The earlier GPT-3.5 Turbo recognized public paragraphs (64% AUROC) more often than private paragraphs (54% AUROC), which suggests that it was trained predominantly on freely available data.\n\nYes, but:\u00a0Newer large language models are better at distinguishing human-written from generated text, even if it wasn\u2019t in their training sets. For instance, given paragraphs that were published after their knowledge cutoffs, GPT-4o returned scores as high as 78 percent AUROC. The authors note that this may challenge their conclusions, since they interpret high scores to indicate that a model saw the text during training. Nonetheless, they argue that their approach will remain valid while\u00a0scores for both text that was included and text that was excluded from training sets\u00a0remain under 96 percent AUROC. \u201cFor now,\u201d they write, \u201cthe gap remains sufficiently large to reliably separate the two categories.\u201d\n\nBehind the news:\u00a0Historically AI developers have trained machine learning models on any data they could acquire. But in the era of generative AI, models trained on copyrighted works can mimic the works and styles of the works\u2019 owners, creating a threat to their livelihoods. Some AI developers have responded by regarding data that\u2019s freely available on the web as fair game, and material that\u2019s otherwise protected as off-limits for training. However, datasets that include ostensibly private data are widely circulated, including\u00a0LibGen, which includes all 34 of the O\u2019Reilly Media titles tested in this study. Moreover, unauthorized copies of many copyrighted works are posted\u00a0without paywalls or even logins, making it possible even\u00a0for web scrapers that crawl only the open web to download them. Google and OpenAI, which is currently embroiled in lawsuits by authors and publishers who claim it violated copyrights by training models on copyrighted works, recently\u00a0lobbied\u00a0the United States government to relax copyright laws for AI developers.\n\nWhy it matters:\u00a0The AI industry requires huge quantities of high-quality data to keep advancing the state of the art. At the same time, copyright owners are worried that models trained on their data might hamper their opportunities to earn a living. AI developers must find fair ways to respond. As O\u2019Reilly points out, exploiting copyrighted works instead of rewarding their authors could lead to an \u201cextractive dead end\u201d that ultimately diminishes the supply of the high-quality training data.\n\nWe\u2019re thinking:\u00a0We have learned a great deal from O\u2019Reilly Media\u2019s books, and we\u2019re grateful to the many authors, editors, graphic artists, and others who produce them. Meanwhile, it\u2019s time for the U.S. Congress \u2014\u00a0 and legislators internationally \u2014 to\u00a0update\u00a0copyright laws for the era of generative AI, so everyone knows the rules and we can find ways to follow them."]}
{"question": "Do we have any news on LA protests?", "answer": "I don't know based on the provided context.", "contexts": ["TEXT:\nA study co-authored by tech-manual publisher Tim O\u2019Reilly shows that OpenAI trained GPT-4o on parts of his company\u2019s books that were not made freely available.\n\nWhat happened:\u00a0O\u2019Reilly, computer scientist Sruly Rosenblat, and economist Ilan Strauss\u00a0found\u00a0that GPT-4o was able to identify verbatim excerpts from dozens of O\u2019Reilly Media books that the company kept behind a paywall, indicating that the books likely were included in the model\u2019s training data.\n\nHow it works:\u00a0The researchers adapted the\u00a0DE-COP\u00a0method to compare how well GPT-4o, GPT-4o-mini, and GPT-3.5 Turbo recognized paywalled excerpts versus freely available excerpts from the same books.\n\nThe team selected 34 O\u2019Reilly Media books and divided them into roughly 14,000 paragraphs.They labeled the paragraphs private (paywalled) or public (when O\u2019Reilly Media publishes a book, it distributes freely on the web chapters 1 and 4 as well as the first 1,500 characters of other chapters). They also labeled the paragraphs according to whether they were published before or after the models\u2019 knowledge cutoff dates.The team built multiple-choice quizzes, each composed of a verbatim paragraph and three paraphrased versions generated by Claude 3.5 Sonnet. The researchers ordered the paragraphs and paraphrases in all permutations to eliminate potential position bias.\n\nResults:\u00a0The authors asked each model to identify the verbatim paragraph and calculated each model\u2019s percentage of correct responses. Then they averaged each model\u2019s accuracy per book and converted the averages into AUROC scores that measure how well a model distinguished books available prior to its knowledge cutoff (potentially included in the training set) from those that weren\u2019t available at the time. 50 percent AUROC indicates random chance, while higher scores indicate higher accuracy.\n\nGPT-4o tended to recognize O\u2019Reilly Media content whether or not it was public, but it recognized private paragraphs (82 percent AUROC) markedly more often than public paragraphs (64 percent AUROC).GPT-4o-mini\u2019s performance was nearly random for both private (56% AUROC) and public material (55% AUROC). The researchers hypothesize that either (i) the model\u2019s smaller size may limit its ability to memorize or (2) OpenAI may reserve premium data to train larger models.The earlier GPT-3.5 Turbo recognized public paragraphs (64% AUROC) more often than private paragraphs (54% AUROC), which suggests that it was trained predominantly on freely available data.\n\nYes, but:\u00a0Newer large language models are better at distinguishing human-written from generated text, even if it wasn\u2019t in their training sets. For instance, given paragraphs that were published after their knowledge cutoffs, GPT-4o returned scores as high as 78 percent AUROC. The authors note that this may challenge their conclusions, since they interpret high scores to indicate that a model saw the text during training. Nonetheless, they argue that their approach will remain valid while\u00a0scores for both text that was included and text that was excluded from training sets\u00a0remain under 96 percent AUROC. \u201cFor now,\u201d they write, \u201cthe gap remains sufficiently large to reliably separate the two categories.\u201d\n\nBehind the news:\u00a0Historically AI developers have trained machine learning models on any data they could acquire. But in the era of generative AI, models trained on copyrighted works can mimic the works and styles of the works\u2019 owners, creating a threat to their livelihoods. Some AI developers have responded by regarding data that\u2019s freely available on the web as fair game, and material that\u2019s otherwise protected as off-limits for training. However, datasets that include ostensibly private data are widely circulated, including\u00a0LibGen, which includes all 34 of the O\u2019Reilly Media titles tested in this study. Moreover, unauthorized copies of many copyrighted works are posted\u00a0without paywalls or even logins, making it possible even\u00a0for web scrapers that crawl only the open web to download them. Google and OpenAI, which is currently embroiled in lawsuits by authors and publishers who claim it violated copyrights by training models on copyrighted works, recently\u00a0lobbied\u00a0the United States government to relax copyright laws for AI developers.\n\nWhy it matters:\u00a0The AI industry requires huge quantities of high-quality data to keep advancing the state of the art. At the same time, copyright owners are worried that models trained on their data might hamper their opportunities to earn a living. AI developers must find fair ways to respond. As O\u2019Reilly points out, exploiting copyrighted works instead of rewarding their authors could lead to an \u201cextractive dead end\u201d that ultimately diminishes the supply of the high-quality training data.\n\nWe\u2019re thinking:\u00a0We have learned a great deal from O\u2019Reilly Media\u2019s books, and we\u2019re grateful to the many authors, editors, graphic artists, and others who produce them. Meanwhile, it\u2019s time for the U.S. Congress \u2014\u00a0 and legislators internationally \u2014 to\u00a0update\u00a0copyright laws for the era of generative AI, so everyone knows the rules and we can find ways to follow them."]}
{"question": "Do we have any news on LA protests?", "answer": "I don't know based on the provided context.", "contexts": [""]}
{"question": "What is new about deepseek v3 and it's perfomance comparing to other models?", "answer": "The new DeepSeek-R1-0528 surpasses its predecessor and approaches the performance of OpenAI o3 and Google Gemini-2.5 Pro. It is a fine-tuned version of DeepSeek-V3 from late 2024 and was exposed to further \u201calgorithmic optimization mechanisms during post-training\u201d. It consumes more tokens at inference. In terms of performance, DeepSeek-R1-0528 nips at the heels of top closed LLMs on a variety of benchmarks. It achieved 17.7 percent of the reasoning problems in HLE compared to the previous version's 8.5 percent. On Aider, it achieved 71.6 percent accuracy compared to the previous version's 53.3 percent accuracy. However, it consumed nearly twice as many tokens. On some tests, it performed ahead of Gemini-2.5 Pro-0506 but behind o3. On others, it fell behind both Gemini-2.5 Pro-0506 and o3. \n\nA smaller version, DeepSeek-R1-0528-Qwen3-8B, runs on a single GPU with as little as 40GB VRAM. It is based on Qwen3-8B with reasoning knowledge distilled from DeepSeek-R1-0528. This model raises the bar for LLMs in its 8-billion-parameter size class. It excelled on AIME 2025, where it achieved 76.3 percent, ahead of the much larger Qwen3-32B (72.9 percent) and just behind o3-mini set to medium effort (76.7 percent). However, it did less well on GPQA and LiveCodeBench.", "contexts": ["TEXT:\nDeepSeek updated its groundbreaking DeepSeek-R1 large language model to strike another blow for open-weights performance.\n\nWhat\u2019s new:\u00a0The new\u00a0DeepSeek-R1-0528\u00a0surpasses its predecessor and approaches the performance of OpenAI o3 and Google Gemini-2.5 Pro. A smaller\u00a0version,\u00a0DeepSeek-R1-0528-Qwen3-8B, runs on a single GPU with as little as 40GB VRAM, according to\u00a0TechCrunch.\n\nInput/output:\u00a0Text in (up to 64,000 tokens), text out (up to 64,000 tokens)Architecture:\u00a0DeepSeek-R1-0528\u00a0mixture-of-experts transformer, 685 billion parameters (upgraded from 671 billion), 37 billion active at any given time;\u00a0DeepSeek-R1-0528-Qwen3-8B\u00a0transformerFeatures:\u00a0JSON output, tool useAvailability/price:\u00a0Both models free via\u00a0Hugging Face\u00a0for noncommercial and commercial uses under\u00a0MIT License, DeepSeek-R1-0528 available via DeepSeek\u2019s app by entering the conversation interface and turning on Deep Thinking,\u00a0DeepSeek API\u00a0$0.14/$2.19 per 1 million tokens of input/output ($0.035/$0.55 per 1 million tokens of input/output from 4:30 P.M. to 12:30 A.M. Pacific Time)Undisclosed:\u00a0Fine-tuning data and methods\n\nHow it works:\u00a0DeepSeek released little information so far about how it built the new models.\n\nLike the original\u00a0DeepSeek-R1, DeepSeek-R1-0528 is a fine-tuned version of\u00a0DeepSeek-V3\u00a0from late 2024. It was exposed to further \u201calgorithmic optimization mechanisms during post-training\u201d and consumes more tokens at inference.DeepSeek-R1-0528-Qwen3-8B is based on Qwen3-8B with reasoning knowledge distilled from DeepSeek-R1-0528.\n\nPerformance:\u00a0DeepSeek-R1-0528 nips at the heels of top closed LLMs on a variety of benchmarks, while DeepSeek-R1-0528-Qwen3-8B raises the bar for LLMs in its 8-billion-parameter size class. DeepSeek claims general improvements in reasoning, managing complex tasks, and writing and editing lengthy prose, along with 50 percent fewer hallucinations when rewriting and summarizing.\n\nDeepSeek-R1-0528 improves on the previous version dramatically in some cases. In DeepSeek\u2019s tests, it achieved 17.7 percent of the reasoning problems in\u00a0HLE\u00a0compared to the previous version's 8.5 percent. On Aider, it achieved 71.6 percent accuracy compared to the previous version's 53.3 percent accuracy, and it made a similar improvement on AIME 2025 (math) \u2014 although it consumed nearly twice as many tokens.On AIME 2024 and AIME 2025 (high-school math competition problems) as well as\u00a0LiveCodeBench\u00a0(coding challenges), DeepSeek-R1-0528 performed ahead of Gemini-2.5 Pro-0506 but behind o3. On GPQA Diamond (graduate-level knowledge in a variety of domains), Aider (programming tasks), and HLE (reasoning), it\u00a0fell behind both Gemini-2.5 Pro-0506 and o3.DeepSeek-R1-0528-Qwen3-8B excelled on AIME 2025, where it achieved 76.3 percent, ahead of the much larger Qwen3-32B (72.9 percent) and just behind o3-mini set to medium effort (76.7 percent). It did less well on GPQA, underperforming\u00a0the other models reported by DeepSeek, and LiveCodeBench, where it fell behind Gemini 2.5-Flash-Thinking-0520.\n\nBehind the news:\u00a0The initial version of DeepSeek-R1 challenged the belief that building top-performing AI models requires tens to hundreds of millions of dollars, top-of-the-line GPUs, and enormous numbers of GPU hours. For the second time in less than a year, DeepSeek has built a competitive LLM with a relatively\u00a0low\u00a0budget.\n\nWhy it matters:\u00a0DeepSeek\u2019s models, along with Alibaba\u2019s Qwen series, continue\u00a0to narrow the gap between open-weights models and their closed peers. Its accomplishments could lead to wider adoption of less-expensive, more-efficient approaches. DeepSeek is passing along the cost savings to developers, offering high-performance inference at a fraction of the cost of closed models.\n\nWe\u2019re thinking:\u00a0DeepSeek-R1-0528-Qwen3-8B mixes contributions from open-weight models \u2014 possible only because Qwen3\u2019s license, like DeepSeek\u2019s is permissive. Open models enable experimentation and innovation in ways that closed models do not."]}
{"question": "What is new about deepseekv3 and its perfomance comparing to other models?", "answer": "DeepSeek-V3 is a new open large language model that outperforms Llama 3.1 405B and GPT-4o on key benchmarks and achieves exceptional scores in coding and math. It is a mixture-of-experts (MoE) transformer that comprises 671 billion parameters, of which 37 billion are active at any moment. The model was trained in less time and at a significantly lower cost than Llama 3.1 405B. It was trained on roughly 15 trillion tokens, including a larger percentage of coding and math data relative to DeepSeek-V2. The model also uses multi-head latent attention, which saves memory during execution relative to other variants of attention. In tests, DeepSeek-V3 outperformed Llama 3.1 405B and Qwen 2.5 72B across the board, and its performance compared favorably with that of GPT-4o. It showed exceptional performance in coding and math tasks, dominating in five of the seven benchmarks tested. However, it lost to o1 on one of the five, according to a public leaderboard.", "contexts": ["TEXT:\nA new model from Hangzhou upstart DeepSeek delivers outstanding performance and may change the equation for training costs.\n\nWhat\u2019s new:\u00a0DeepSeek-V3\u00a0is an open large language model that outperforms Llama 3.1 405B and GPT-4o on key benchmarks and achieves exceptional scores in coding and math. The weights are\u00a0open\u00a0except for applications that involve military uses, harming minors, generating false information, and similar restrictions. You can download them\u00a0here.\n\nMixture of experts (MoE) basics:\u00a0The MoE architecture uses different subsets of its parameters to process different inputs. Each MoE layer contains a group of neural networks, or experts, preceded by a gating module that learns to choose which one(s) to use based on the input. In this way, different experts learn to specialize in different types of examples. Because not all parameters are used to produce any given output, the network uses less energy and runs faster than models of similar size that use all parameters to process every input.\n\nHow it works:\u00a0DeepSeek-V3 is a mixture-of-experts (MoE) transformer that comprises 671 billion parameters, of which 37 billion are active at any moment. The team trained the model in 2.79 million GPU hours \u2014 less than 1/10 the\u00a0time required to train Llama 3.1 405B, which DeepSeek-V3 substantially outperforms \u2014 at an extraordinarily low cost of $5.6 million.\n\nThe developers trained it on roughly 15 trillion tokens, including a larger percentage of coding and math data relative to DeepSeek-V2. They fine-tuned it on a wide variety of tasks using output generated by\u00a0DeepSeek-R1\u00a0and DeepSeek-V2.5. They further sharpened its performance across diverse domains using the reinforcement learning algorithm known as\u00a0group relative policy optimization. \u00a0Earlier\u00a0work\u00a0showed that training to predict the next two tokens would improve performance over learning to predict just one. The authors implemented this procedure. The model learned to predict the first token as usual and used an additional set of layers to learn to predict the second token. The additional layers aren\u2019t used at inference.Following\u00a0DeepSeek-V2, DeepSeek-V3 uses multi-head latent attention, which saves memory during execution relative to other variants of attention.Also like DeepSeek-V2, the new model combines dedicated (routed) and shared experts. The model chooses eight of 256 experts for a particular input, but it also uses a shared expert that processes all inputs.\n\nResults:\u00a0In DeepSeek\u2019s tests, DeepSeek-V3 outperformed Llama 3.1 405B and Qwen 2.5 72B across the board, and its performance compared favorably with that of GPT-4o.\n\nDeepSeek-V3 showed exceptional performance in coding and math tasks. In coding, DeepSeek-V3 dominated in five of the seven benchmarks tested. However, DeepSeek-V3 lost to o1 on one of the five, according to a public leaderboard. Specifically, on\u00a0Polyglot, which tests a model\u2019s ability to generate code in response to difficult requests in multiple programming languages, DeepSeek-V3 (48.5 percent accuracy) beat Claude Sonnet 3.5 (45.3 percent accuracy), though it lost to o1 (61.7 percent accuracy).\u00a0In language tasks, it performed neck-and-neck with Claude 3.5 Sonnet, achieving higher scores in some tasks and lower in others.\n\nBehind the news:\u00a0\u00a0OpenAI\u2019s o1 models excel thanks to agentic workflows in which they reflect on their own outputs, use tools, and so on. DeepSeek swims against the tide and achieves superior results without relying on agentic workflows.\n\nWhy it matters:\u00a0Open models continue to challenge closed models, giving developers high-quality options that they can modify and deploy at will. But the larger story is DeepSeek-V3\u2019s shockingly low training cost.\u00a0The team doesn\u2019t explain precisely how the model achieves outstanding performance with such a low processing budget. (The paper credits \u201cmeticulous engineering optimizations.\u201d) But it\u2019s likely that DeepSeek\u2019s steady refinement of MoE is a key factor. DeepSeek-V2, also an MoE model, saved more than 40 percent in training versus the earlier DeepSeek 67B, which didn\u2019t employ MoE. In 2022,\u00a0Microsoft\u00a0found that MoE cost five times less in training for equal performance compared to a dense model, and\u00a0Google\u00a0and\u00a0Meta\u00a0reported that MoE achieved better performance than dense models trained on the same numbers of tokens.\n\nWe\u2019re thinking:\u00a0If they can be replicated, DeepSeek\u2019s results have significant implications for the economics of training foundation models. If indeed it now costs around $5 million to build a GPT-4o-level model, more teams will be able to train such models, and the cost of competing with the AI giants could fall dramatically."]}
{"question": "What is new about DeepSeek-v3 and how it compares to other models?", "answer": "The new DeepSeek-R1-0528 surpasses its predecessor and approaches the performance of OpenAI o3 and Google Gemini-2.5 Pro. It is a fine-tuned version of DeepSeek-V3 from late 2024 and was exposed to further \u201calgorithmic optimization mechanisms during post-training\u201d. It consumes more tokens at inference and nips at the heels of top closed LLMs on a variety of benchmarks. A smaller version, DeepSeek-R1-0528-Qwen3-8B, runs on a single GPU with as little as 40GB VRAM. This smaller version raises the bar for LLMs in its 8-billion-parameter size class and is based on Qwen3-8B with reasoning knowledge distilled from DeepSeek-R1-0528. Both models are free via Hugging Face for noncommercial and commercial uses under MIT License.", "contexts": ["TEXT:\nDeepSeek updated its groundbreaking DeepSeek-R1 large language model to strike another blow for open-weights performance.\n\nWhat\u2019s new:\u00a0The new\u00a0DeepSeek-R1-0528\u00a0surpasses its predecessor and approaches the performance of OpenAI o3 and Google Gemini-2.5 Pro. A smaller\u00a0version,\u00a0DeepSeek-R1-0528-Qwen3-8B, runs on a single GPU with as little as 40GB VRAM, according to\u00a0TechCrunch.\n\nInput/output:\u00a0Text in (up to 64,000 tokens), text out (up to 64,000 tokens)Architecture:\u00a0DeepSeek-R1-0528\u00a0mixture-of-experts transformer, 685 billion parameters (upgraded from 671 billion), 37 billion active at any given time;\u00a0DeepSeek-R1-0528-Qwen3-8B\u00a0transformerFeatures:\u00a0JSON output, tool useAvailability/price:\u00a0Both models free via\u00a0Hugging Face\u00a0for noncommercial and commercial uses under\u00a0MIT License, DeepSeek-R1-0528 available via DeepSeek\u2019s app by entering the conversation interface and turning on Deep Thinking,\u00a0DeepSeek API\u00a0$0.14/$2.19 per 1 million tokens of input/output ($0.035/$0.55 per 1 million tokens of input/output from 4:30 P.M. to 12:30 A.M. Pacific Time)Undisclosed:\u00a0Fine-tuning data and methods\n\nHow it works:\u00a0DeepSeek released little information so far about how it built the new models.\n\nLike the original\u00a0DeepSeek-R1, DeepSeek-R1-0528 is a fine-tuned version of\u00a0DeepSeek-V3\u00a0from late 2024. It was exposed to further \u201calgorithmic optimization mechanisms during post-training\u201d and consumes more tokens at inference.DeepSeek-R1-0528-Qwen3-8B is based on Qwen3-8B with reasoning knowledge distilled from DeepSeek-R1-0528.\n\nPerformance:\u00a0DeepSeek-R1-0528 nips at the heels of top closed LLMs on a variety of benchmarks, while DeepSeek-R1-0528-Qwen3-8B raises the bar for LLMs in its 8-billion-parameter size class. DeepSeek claims general improvements in reasoning, managing complex tasks, and writing and editing lengthy prose, along with 50 percent fewer hallucinations when rewriting and summarizing.\n\nDeepSeek-R1-0528 improves on the previous version dramatically in some cases. In DeepSeek\u2019s tests, it achieved 17.7 percent of the reasoning problems in\u00a0HLE\u00a0compared to the previous version's 8.5 percent. On Aider, it achieved 71.6 percent accuracy compared to the previous version's 53.3 percent accuracy, and it made a similar improvement on AIME 2025 (math) \u2014 although it consumed nearly twice as many tokens.On AIME 2024 and AIME 2025 (high-school math competition problems) as well as\u00a0LiveCodeBench\u00a0(coding challenges), DeepSeek-R1-0528 performed ahead of Gemini-2.5 Pro-0506 but behind o3. On GPQA Diamond (graduate-level knowledge in a variety of domains), Aider (programming tasks), and HLE (reasoning), it\u00a0fell behind both Gemini-2.5 Pro-0506 and o3.DeepSeek-R1-0528-Qwen3-8B excelled on AIME 2025, where it achieved 76.3 percent, ahead of the much larger Qwen3-32B (72.9 percent) and just behind o3-mini set to medium effort (76.7 percent). It did less well on GPQA, underperforming\u00a0the other models reported by DeepSeek, and LiveCodeBench, where it fell behind Gemini 2.5-Flash-Thinking-0520.\n\nBehind the news:\u00a0The initial version of DeepSeek-R1 challenged the belief that building top-performing AI models requires tens to hundreds of millions of dollars, top-of-the-line GPUs, and enormous numbers of GPU hours. For the second time in less than a year, DeepSeek has built a competitive LLM with a relatively\u00a0low\u00a0budget.\n\nWhy it matters:\u00a0DeepSeek\u2019s models, along with Alibaba\u2019s Qwen series, continue\u00a0to narrow the gap between open-weights models and their closed peers. Its accomplishments could lead to wider adoption of less-expensive, more-efficient approaches. DeepSeek is passing along the cost savings to developers, offering high-performance inference at a fraction of the cost of closed models.\n\nWe\u2019re thinking:\u00a0DeepSeek-R1-0528-Qwen3-8B mixes contributions from open-weight models \u2014 possible only because Qwen3\u2019s license, like DeepSeek\u2019s is permissive. Open models enable experimentation and innovation in ways that closed models do not."]}
{"question": "What is we compare DeepSeek R1 to other models?", "answer": "The context does not provide any information on how DeepSeek R1 compares to other models.", "contexts": ["TEXT:\nA new model from Hangzhou upstart DeepSeek delivers outstanding performance and may change the equation for training costs.\n\nWhat\u2019s new:\u00a0DeepSeek-V3\u00a0is an open large language model that outperforms Llama 3.1 405B and GPT-4o on key benchmarks and achieves exceptional scores in coding and math. The weights are\u00a0open\u00a0except for applications that involve military uses, harming minors, generating false information, and similar restrictions. You can download them\u00a0here.\n\nMixture of experts (MoE) basics:\u00a0The MoE architecture uses different subsets of its parameters to process different inputs. Each MoE layer contains a group of neural networks, or experts, preceded by a gating module that learns to choose which one(s) to use based on the input. In this way, different experts learn to specialize in different types of examples. Because not all parameters are used to produce any given output, the network uses less energy and runs faster than models of similar size that use all parameters to process every input.\n\nHow it works:\u00a0DeepSeek-V3 is a mixture-of-experts (MoE) transformer that comprises 671 billion parameters, of which 37 billion are active at any moment. The team trained the model in 2.79 million GPU hours \u2014 less than 1/10 the\u00a0time required to train Llama 3.1 405B, which DeepSeek-V3 substantially outperforms \u2014 at an extraordinarily low cost of $5.6 million.\n\nThe developers trained it on roughly 15 trillion tokens, including a larger percentage of coding and math data relative to DeepSeek-V2. They fine-tuned it on a wide variety of tasks using output generated by\u00a0DeepSeek-R1\u00a0and DeepSeek-V2.5. They further sharpened its performance across diverse domains using the reinforcement learning algorithm known as\u00a0group relative policy optimization. \u00a0Earlier\u00a0work\u00a0showed that training to predict the next two tokens would improve performance over learning to predict just one. The authors implemented this procedure. The model learned to predict the first token as usual and used an additional set of layers to learn to predict the second token. The additional layers aren\u2019t used at inference.Following\u00a0DeepSeek-V2, DeepSeek-V3 uses multi-head latent attention, which saves memory during execution relative to other variants of attention.Also like DeepSeek-V2, the new model combines dedicated (routed) and shared experts. The model chooses eight of 256 experts for a particular input, but it also uses a shared expert that processes all inputs.\n\nResults:\u00a0In DeepSeek\u2019s tests, DeepSeek-V3 outperformed Llama 3.1 405B and Qwen 2.5 72B across the board, and its performance compared favorably with that of GPT-4o.\n\nDeepSeek-V3 showed exceptional performance in coding and math tasks. In coding, DeepSeek-V3 dominated in five of the seven benchmarks tested. However, DeepSeek-V3 lost to o1 on one of the five, according to a public leaderboard. Specifically, on\u00a0Polyglot, which tests a model\u2019s ability to generate code in response to difficult requests in multiple programming languages, DeepSeek-V3 (48.5 percent accuracy) beat Claude Sonnet 3.5 (45.3 percent accuracy), though it lost to o1 (61.7 percent accuracy).\u00a0In language tasks, it performed neck-and-neck with Claude 3.5 Sonnet, achieving higher scores in some tasks and lower in others.\n\nBehind the news:\u00a0\u00a0OpenAI\u2019s o1 models excel thanks to agentic workflows in which they reflect on their own outputs, use tools, and so on. DeepSeek swims against the tide and achieves superior results without relying on agentic workflows.\n\nWhy it matters:\u00a0Open models continue to challenge closed models, giving developers high-quality options that they can modify and deploy at will. But the larger story is DeepSeek-V3\u2019s shockingly low training cost.\u00a0The team doesn\u2019t explain precisely how the model achieves outstanding performance with such a low processing budget. (The paper credits \u201cmeticulous engineering optimizations.\u201d) But it\u2019s likely that DeepSeek\u2019s steady refinement of MoE is a key factor. DeepSeek-V2, also an MoE model, saved more than 40 percent in training versus the earlier DeepSeek 67B, which didn\u2019t employ MoE. In 2022,\u00a0Microsoft\u00a0found that MoE cost five times less in training for equal performance compared to a dense model, and\u00a0Google\u00a0and\u00a0Meta\u00a0reported that MoE achieved better performance than dense models trained on the same numbers of tokens.\n\nWe\u2019re thinking:\u00a0If they can be replicated, DeepSeek\u2019s results have significant implications for the economics of training foundation models. If indeed it now costs around $5 million to build a GPT-4o-level model, more teams will be able to train such models, and the cost of competing with the AI giants could fall dramatically."]}
{"question": "I want to know about US limiting export of electronics to China", "answer": "The United States has proposed new rules that would limit the export of AI technology, including advanced chips and models, to most countries beyond a select group of close allies. These rules would create a three-tier system, with China falling into Tier 3. This means that China, along with Russia and around two dozen other countries, would be blocked from receiving advanced AI chips, model weights, and specialized knowledge related to these systems. This builds on previous restrictions, such as those in 2022 that barred semiconductor suppliers AMD and Nvidia from selling advanced chips to Chinese firms. In November 2024, the U.S. tightened restrictions further, ordering Taiwan Semiconductor Manufacturing Company to halt production of advanced chips destined for China.", "contexts": ["TEXT:\nThe United States proposed limits on exports of AI technology that would dramatically expand previous restrictions, creating a new international hierarchy for access to advanced chips and models.\n\nWhat\u2019s new:\u00a0The Biden administration, which will transition to leadership under incoming President Trump next week, issued new\u00a0rules\u00a0that restrict exports of AI chips and models to most countries beyond a select group of close allies. The rules, which are not yet final, would create a three-tier system that limits exports to a number of close allies and blocks access entirely to China, Iran, North Korea, Russia, and others. They also would introduce the U.S.\u2019 first-ever restrictions on exporting closed weights for large AI models.\n\nHow it works:\u00a0The restrictions were announced shortly after a\u00a0leak\u00a0reached the press. A public comment period of 120 days will enable the incoming U.S. Presidential administration to consider input from the business and diplomatic communities and modify the rules before they take effect. The rules are scheduled to take effect in one year.\n\nA new\u00a0hierarchy\u00a0divides nations into three groups that would have different degrees of access to AI chips both designed in the U.S. and manufactured abroad using U.S. technology, as well as proprietary AI models.Tier 1:\u00a0Australia, Japan, Taiwan, the United Kingdom, and most of Europe would retain nearly unrestricted access. However, these nations must keep 75 percent of their AI computing power within allied countries. No more than 10 percent can be transferred to any single country outside this group to ensure that advanced AI development remains concentrated among close U.S. allies.Tier 2:\u00a0Traditional U.S. allies and trade partners like Israel, Saudi Arabia, and Singapore face an initial cap of 507 million units of total processing power (TPP) \u2014 roughly the computational capacity of 32,000 Nvidia H100 chips \u2014 through the first quarter of 2025. The cap would increase to 1.02 billion TPP by 2027. U.S. companies that operate in these countries can apply for higher limits: 633 million TPP in Q1 2025, rising to 5.064 billion TPP by Q1 2027.Tier 3:\u00a0China, Russia, and around two dozen other countries are blocked from receiving advanced AI chips, model weights, and specialized knowledge related to these systems.The U.S. Commerce Department\u2019s export control agency must approve the export of models or transfer of weights of closed models that were trained using more than 1026\u00a0computational operations. These rules target future systems, as no known models today used this amount of computation during training.Companies based in the U.S. must maintain at least 50 percent of their total AI computing power within U.S. borders. They also must track distribution of their models, implement security measures, and submit to regular audits.\n\nBehind the news:\u00a0The proposed rules build on 2022\u2019s\u00a0CHIPS and Science Act, which was designed to strengthen domestic semiconductor production and restrict technologies abroad that could bear on U.S. security. An initial round of restrictions in late 2022\u00a0barred\u00a0semiconductor suppliers AMD and Nvidia from selling advanced chips to Chinese firms. In November 2024, the U.S.\u00a0tightened\u00a0restrictions further, ordering Taiwan Semiconductor Manufacturing Company, which fabricates those chips, to halt production of advanced chips destined for China.\n\nPlus green AI infrastructure:\u00a0In addition, President Biden issued an executive order to encourage the rapid build-out of computing infrastructure for AI. The federal government will hold competitions among private companies to lease sites it owns for the building of data centers at private expense. The selection of sites will take into account availability of sources of clean energy, including support for nuclear energy. The government will expedite permitting on these sites and support development of energy transmission lines around them. It will also encourage international allies to invest in AI infrastructure powered by clean energy.\n\nWhy it matters:\u00a0Protecting the United States\u2019 advantages in high tech has been a rising priority for the White House over the past decade.\u00a0The earlier export restrictions forced many Chinese AI developers to rely on less-powerful hardware. The new limits are likely to have a far broader impact. They could force developers in the Tier 2 and Tier 3 countries to build less resource-intensive models and lead them to collaborate more closely with each other, reducing the value of U.S.-made technology worldwide. They could hurt U.S. chip vendors, which have\u00a0warned\u00a0that the rules could weaken U.S. competitiveness in the global economy. They could also force companies that are building huge data centers to process AI calculations to\u00a0reconsider\u00a0their plans.\n\nWe\u2019re thinking:\u00a0The Biden administration\u2019s embargo on AI chips has been\u00a0leaky. So far, it has slowed down adversaries only slightly while spurring significant investment in potential\u00a0suppliers\u00a0that aren\u2019t connected to the U.S. While the public comment period invites lobbying and industry feedback, ultimately geopolitical priorities may hold sway. Whatever the outcome, reducing the world\u2019s dependence on U.S. chips and models would result in a very different global AI ecosystem."]}
{"question": "What is new about protests in California?", "answer": "I don't know based on the provided context.", "contexts": [""]}
